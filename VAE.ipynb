{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VAE Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **i. Imports and subfunctions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "def split_iid(dataset, n_centers):\n",
    "    \"\"\" Split PyTorch dataset randomly into n_centers.\"\"\"\n",
    "    n_obs_per_center = [len(dataset) // n_centers for _ in range(n_centers)]\n",
    "    return random_split(dataset, n_obs_per_center)\n",
    "\n",
    "def federated_averaging(models, n_obs_per_client):\n",
    "    \"\"\" Perform federated averaging.\"\"\"\n",
    "    # Error check inputs\n",
    "    assert len(models) > 0\n",
    "    assert len(n_obs_per_client) == len(models)\n",
    "\n",
    "    # Compute proportions\n",
    "    n_obs = sum(n_obs_per_client)\n",
    "    proportions = [n_k / n_obs for n_k in n_obs_per_client]\n",
    "\n",
    "    # Empty model parameter dictionary\n",
    "    avg_params = models[0].state_dict()\n",
    "    for key, val in avg_params.items():\n",
    "        avg_params[key] = torch.zeros_like(val)\n",
    "\n",
    "    # Compute average\n",
    "    for model, proportion in zip(models, proportions):\n",
    "        for key in avg_params.keys():\n",
    "            avg_params[key] += proportion * model.state_dict()[key]\n",
    "\n",
    "    # Copy one of the models and load trained params\n",
    "    avg_model = copy.deepcopy(models[0])\n",
    "    avg_model.load_state_dict(avg_params)\n",
    "\n",
    "    return avg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ii. General setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /homes/hd119/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /homes/hd119/data/MNIST/raw/train-images-idx3-ubyte.gz to /homes/hd119/data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /homes/hd119/data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting /homes/hd119/data/MNIST/raw/train-labels-idx1-ubyte.gz to /homes/hd119/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /homes/hd119/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting /homes/hd119/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /homes/hd119/data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /homes/hd119/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting /homes/hd119/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /homes/hd119/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "82.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /homes/hd119/torch_datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /homes/hd119/torch_datasets/MNIST/raw/train-images-idx3-ubyte.gz to /homes/hd119/torch_datasets/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /homes/hd119/torch_datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting /homes/hd119/torch_datasets/MNIST/raw/train-labels-idx1-ubyte.gz to /homes/hd119/torch_datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /homes/hd119/torch_datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting /homes/hd119/torch_datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to /homes/hd119/torch_datasets/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /homes/hd119/torch_datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting /homes/hd119/torch_datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to /homes/hd119/torch_datasets/MNIST/raw\n",
      "\n",
      "Number of centers: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "N_ROUNDS = 10\n",
    "N_CENTRES = 4\n",
    "N_EPOCHS = 2\n",
    "N_FEATURES = 784\n",
    "batch_size = 1\n",
    "\n",
    "# GPU settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Load data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0,), (1,))])\n",
    "dataset = datasets.MNIST('~/data/', train=True, download=True, transform=transform)\n",
    "train_dataset = datasets.MNIST(root=\"~/torch_datasets\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"~/torch_datasets\", train=False, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "# Federate data\n",
    "federated_dataset = split_iid(train_dataset, n_centers=4)\n",
    "print('Number of centers:', len(federated_dataset))\n",
    "\n",
    "# Put data into dataloader\n",
    "for i, dataset in enumerate(federated_dataset):\n",
    "    federated_dataset[i] = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                       shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Create models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_features,\n",
    "                 latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Assign variables\n",
    "        self.in_features = in_features\n",
    "        self.latent_dim = latent_dim\n",
    "        modules = []\n",
    "\n",
    "        # Build Encoder\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.LeakyReLU())\n",
    "        )\n",
    "\n",
    "        # Build encoder\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(in_features, latent_dim)\n",
    "        self.fc_var = nn.Linear(in_features, latent_dim)\n",
    "\n",
    "        # Build decoder\n",
    "        modules = []\n",
    "        self.decoder_input = nn.Linear(latent_dim, in_features)\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.LeakyReLU())\n",
    "        )\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\" Encode input to mean and logvar.\"\"\"\n",
    "        # Get values from encoding features\n",
    "        result = self.encoder(x)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        logvar = self.fc_var(result)\n",
    "\n",
    "        return [mu, logvar]\n",
    "    \n",
    "    def reparameterise(self, mu, logvar):\n",
    "        \"\"\" Reparameterise to sample.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\" Decode latent sampling to output.\"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = self.decoder(result)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass of the network.\"\"\"\n",
    "        # Check input length matches in_features\n",
    "        if x.shape[-1] != self.in_features:\n",
    "            raise Exception(f'Input features must be equal to {self.in_features}!')\n",
    "\n",
    "        # Encode input to mean and logvar\n",
    "        mu, logvar = self.encode(x)\n",
    "\n",
    "        # Reparameterise\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "\n",
    "        # Decode and return values\n",
    "        return  [self.decode(z), x, mu, logvar]\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder_hidden_layer = nn.Linear(\n",
    "            in_features=kwargs[\"input_shape\"], out_features=128\n",
    "        )\n",
    "        self.encoder_output_layer = nn.Linear(\n",
    "            in_features=128, out_features=128\n",
    "        )\n",
    "        self.decoder_hidden_layer = nn.Linear(\n",
    "            in_features=128, out_features=128\n",
    "        )\n",
    "        self.decoder_output_layer = nn.Linear(\n",
    "            in_features=128, out_features=kwargs[\"input_shape\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        activation = self.encoder_hidden_layer(features)\n",
    "        activation = torch.relu(activation)\n",
    "        code = self.encoder_output_layer(activation)\n",
    "        code = torch.relu(code)\n",
    "        activation = self.decoder_hidden_layer(code)\n",
    "        activation = torch.relu(activation)\n",
    "        activation = self.decoder_output_layer(activation)\n",
    "        reconstructed = torch.relu(activation)\n",
    "        return reconstructed\n",
    "    \n",
    "class VAELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAELoss, self).__init__()\n",
    "        self.criterion = F.mse_loss\n",
    "\n",
    "    def forward(self, inputs, outputs, mu, logvar, Beta=5):   \n",
    "        recons_loss = self.criterion(outputs, inputs)\n",
    "        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return recons_loss + (Beta*kl) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Train VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters is: 12554\n",
      "VAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=784, out_features=5, bias=True)\n",
      "  (fc_var): Linear(in_features=784, out_features=5, bias=True)\n",
      "  (decoder_input): Linear(in_features=5, out_features=784, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialise model\n",
    "StandardVAE = VAE(N_FEATURES, 5).to(device)\n",
    "params = sum(p.numel() for p in StandardVAE.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters is: {}\".format(params))\n",
    "print(StandardVAE)\n",
    "\n",
    "# Optimiser and loss\n",
    "optimizer = torch.optim.Adam(StandardVAE.parameters(), lr=1e-3)\n",
    "criterion = VAELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning standard VAE training...\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m \u001b[39m# Compute reconstructions\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m results \u001b[39m=\u001b[39m StandardVAE(batch_tensor)\n\u001b[1;32m     16\u001b[0m \u001b[39m# Compute training reconstruction loss\u001b[39;00m\n\u001b[1;32m     17\u001b[0m train_loss \u001b[39m=\u001b[39m criterion(batch_tensor, results[\u001b[39m0\u001b[39m], results[\u001b[39m2\u001b[39m], results[\u001b[39m3\u001b[39m])\n",
      "File \u001b[0;32m/vol/bitbucket/hd119/venvs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 64\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput features must be equal to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features\u001b[39m}\u001b[39;00m\u001b[39m!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[39m# Encode input to mean and logvar\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m mu, logvar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(x)\n\u001b[1;32m     66\u001b[0m \u001b[39m# Reparameterise\u001b[39;00m\n\u001b[1;32m     67\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterise(mu, logvar)\n",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m, in \u001b[0;36mVAE.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(result, start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39m# Split the result into mu and var components\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# of the latent Gaussian distribution\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m mu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc_mu(result)\n\u001b[1;32m     40\u001b[0m logvar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_var(result)\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m [mu, logvar]\n",
      "File \u001b[0;32m/vol/bitbucket/hd119/venvs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/vol/bitbucket/hd119/venvs/pytorch/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "# Initialise model\n",
    "StandardVAE.train()\n",
    "print('Beginning standard VAE training...\\n')\n",
    "for epoch in range(N_EPOCHS): \n",
    "    running_loss = 0.0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Batch tensor\n",
    "        batch_tensor = batch[0].view(1, 784).to(device)\n",
    "\n",
    "        # Reset the gradients back to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute reconstructions\n",
    "        results = StandardVAE(batch_tensor)\n",
    "        \n",
    "        # Compute training reconstruction loss\n",
    "        train_loss = criterion(batch_tensor, results[0], results[2], results[3])\n",
    "\n",
    "        # Compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # Perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update lossLoss\n",
    "        running_loss += train_loss.item()\n",
    "\n",
    "        # Loss\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f'Epoch: {epoch} - Batch: {batch_idx} - Loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model paramters and architecture\n",
    "MyEncoder = AutoEncoder(input_shape=784).to(device)\n",
    "optimizer = torch.optim.Adam(MyEncoder.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Copy models over centres\n",
    "encoders = [copy.deepcopy(MyEncoder) for _ in range(N_CENTRES)]\n",
    "n_obs_per_client = [len(client_data) for client_data in federated_dataset]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a3ae10853c86ef1ef65c7de7568f8392afa8d43a76b16d28e927fd429292725"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
