{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Federated Variational Autoencoder**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **i. Imports and subfunctions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ic-ai4health-fri/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/ic-ai4health-fri/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           uuid                     name  \\\n",
      "index                                                                      \n",
      "0      GPU-2a01c044-2e5a-72b5-e75b-f0c9cfaed71d  NVIDIA GeForce RTX 3080   \n",
      "\n",
      "      temperature.gpu utilization.gpu memory.used memory.total  \n",
      "index                                                           \n",
      "0                  58              34         786        10240  \n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import copy\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from mcvae.models import Mcvae, ThreeLayersVAE, VAE\n",
    "\n",
    "# Subfunctions\n",
    "def split_iid(dataset, n_centers):\n",
    "    \"\"\"\n",
    "    Split PyTorch dataset randomly into n_centers\n",
    "    \"\"\"\n",
    "    n_obs_per_center = [len(dataset) // n_centers for _ in range(n_centers)]\n",
    "    return random_split(dataset, n_obs_per_center)\n",
    "\n",
    "def federated_averaging(models, n_obs_per_client):\n",
    "    \"\"\"\n",
    "    Perform federated averaging.\n",
    "    \"\"\"\n",
    "    # Error check inputs\n",
    "    assert len(models) > 0\n",
    "    assert len(n_obs_per_client) == len(models)\n",
    "\n",
    "    # Compute proportions\n",
    "    n_obs = sum(n_obs_per_client)\n",
    "    proportions = [n_k / n_obs for n_k in n_obs_per_client]\n",
    "\n",
    "    # Empty model parameter dictionary\n",
    "    avg_params = models[0].state_dict()\n",
    "    for key, val in avg_params.items():\n",
    "        avg_params[key] = torch.zeros_like(val)\n",
    "\n",
    "    # Compute average\n",
    "    for model, proportion in zip(models, proportions):\n",
    "        for key in avg_params.keys():\n",
    "            avg_params[key] += proportion * model.state_dict()[key]\n",
    "\n",
    "    # Copy one of the models and load trained params\n",
    "    avg_model = copy.deepcopy(models[0])\n",
    "    avg_model.load_state_dict(avg_params)\n",
    "\n",
    "    return avg_model\n",
    "\n",
    "def get_data(subset, shuffle=True):\n",
    "    \"\"\"\n",
    "    Extracts data from a Subset torch dataset in the form of a tensor.\n",
    "    \"\"\"\n",
    "    loader = DataLoader(subset, batch_size=len(subset), shuffle=shuffle)\n",
    "    return next(iter(loader)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ii. General setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define federated parameters\n",
    "N_CENTERS = 4\n",
    "N_ROUNDS = 10\n",
    "\n",
    "# Define learning parameters\n",
    "N_EPOCHS = 15\n",
    "BATCH_SIZE = 48\n",
    "LR = 1e-3\n",
    "\n",
    "# Define device to use for torch\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "else:\n",
    "    use_cuda = False\n",
    "use_cuda = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of centers: 4\n"
     ]
    }
   ],
   "source": [
    "# Define data transforms and download\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0,), (1,))])\n",
    "dataset = datasets.MNIST('~/data/', train=True, download=True, transform=transform)\n",
    "\n",
    "# Federate data\n",
    "federated_dataset = split_iid(dataset, n_centers=N_CENTERS)\n",
    "print('Number of centers:', len(federated_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Create models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature dimensions and dummy data\n",
    "N_FEATURES = 784\n",
    "dummy_data = [torch.zeros(1, N_FEATURES)]\n",
    "\n",
    "# Model architecture\n",
    "lat_dim = 3\n",
    "vae_class = ThreeLayersVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE models\n",
    "model = Mcvae(data=dummy_data, lat_dim=lat_dim, vaeclass=vae_class)\n",
    "model.optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "model.init_loss()\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "models = [copy.deepcopy(model) for _ in range(N_CENTERS)]\n",
    "n_obs_per_client = [len(client_data) for client_data in federated_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(subset, shuffle=True):\n",
    "    \"\"\" Extracts data from a Subset torch dataset in the form of a tensor\"\"\"\n",
    "    loader = DataLoader(subset, batch_size=len(subset), shuffle=shuffle)\n",
    "    return next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[39m# Load client's model parameters and train\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     client_model\u001b[39m.\u001b[39mload_state_dict(init_params)\n\u001b[0;32m---> 17\u001b[0m     client_model\u001b[39m.\u001b[39;49moptimize(epochs\u001b[39m=\u001b[39;49mN_EPOCHS, data\u001b[39m=\u001b[39;49mclient_model\u001b[39m.\u001b[39;49mdata)\n\u001b[1;32m     19\u001b[0m \u001b[39m# Aggregate models using federated averaging\u001b[39;00m\n\u001b[1;32m     20\u001b[0m trained_model \u001b[39m=\u001b[39m federated_averaging(models, n_obs_per_client)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/mcvae/models/utils.py:310\u001b[0m, in \u001b[0;36mUtilities.optimize\u001b[0;34m(self, epochs, data)\u001b[0m\n\u001b[1;32m    308\u001b[0m \t\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_minibatch_loss()\n\u001b[1;32m    309\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 310\u001b[0m \tloss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimize_batch(data)\n\u001b[1;32m    312\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m \t\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstep(loss)  \u001b[39m# For \"ReduceLROnPlateau\" scheduler\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/mcvae/models/utils.py:277\u001b[0m, in \u001b[0;36mUtilities.optimize_batch\u001b[0;34m(self, local_batch)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize_batch\u001b[39m(\u001b[39mself\u001b[39m, local_batch):\n\u001b[0;32m--> 277\u001b[0m \tfwd_return \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(local_batch)\n\u001b[1;32m    278\u001b[0m \tloss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_function(fwd_return)\n\u001b[1;32m    279\u001b[0m \t\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/mcvae/models/mcvae.py:131\u001b[0m, in \u001b[0;36mMcvae.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 131\u001b[0m \tq \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(x)\n\u001b[1;32m    132\u001b[0m \tz \u001b[39m=\u001b[39m [_\u001b[39m.\u001b[39mrsample() \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m q]\n\u001b[1;32m    133\u001b[0m \tp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(z)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/mcvae/models/mcvae.py:117\u001b[0m, in \u001b[0;36mMcvae.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 117\u001b[0m \t\u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvae[i]\u001b[39m.\u001b[39mencode(x[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_channels)]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/mcvae/models/mcvae.py:117\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 117\u001b[0m \t\u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvae[i]\u001b[39m.\u001b[39;49mencode(x[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_channels)]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/mcvae/models/vae.py:81\u001b[0m, in \u001b[0;36mVAE.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m:param x: list of datasets (Obs x Feats)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39m:return: list of encoded distributions (one list element per dataset)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 81\u001b[0m mu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_mu(x)\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparse:\n\u001b[1;32m     83\u001b[0m \tlogvar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_logvar(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_mm)"
     ]
    }
   ],
   "source": [
    "# Initialise paramters\n",
    "init_params = model.state_dict()\n",
    "\n",
    "# Loop over training rounds and clients\n",
    "for round_i in range(N_ROUNDS):\n",
    "    for client_dataset, client_model in zip(federated_dataset, models):\n",
    "        # Load client data in the form of a tensor\n",
    "        X, y = get_data(client_dataset)\n",
    "        if use_cuda:\n",
    "            X.cuda()\n",
    "            y.cuda()\n",
    "        client_model.data = X.view(-1, N_FEATURES)  # Set data attribute in client's model (list wraps the number of channels)\n",
    "        print(client_model)\n",
    "\n",
    "        # Load client's model parameters and train\n",
    "        client_model.load_state_dict(init_params)\n",
    "        client_model.optimize(epochs=N_EPOCHS, data=client_model.data)\n",
    "        \n",
    "    # Aggregate models using federated averaging\n",
    "    trained_model = federated_averaging(models, n_obs_per_client)\n",
    "    init_params = trained_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "badda3f1d1321150ebb1865afb1bcc449740e2ccf220bb3debb2fa46dbffb0c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
